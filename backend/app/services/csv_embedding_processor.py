"""
CSV Embedding Processor for Colleague's Script Output

This module processes CSV files generated by the colleague's embedding script
and imports the embeddings directly into PostgreSQL with pgvector.

Expected CSV format from colleague:
document_id,embedding,model_name,vector_dim,...
uuid-123,"[0.1,0.2,0.3,...]",sentence-transformers/all-MiniLM-L12-v2,384
uuid-456,"[0.4,0.5,0.6,...]",sentence-transformers/all-MiniLM-L12-v2,384
"""

import csv
import json
import uuid
import os
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import numpy as np
from sqlalchemy.orm import Session
from sqlalchemy import text

from ..database import get_session
from ..models.embedding import Embedding
from ..models.document import Document


class CSVEmbeddingProcessor:
    """Processes CSV files with embeddings generated by colleague's script."""
    
    def __init__(self, session: Optional[Session] = None):
        self.session = session
        self.processed_files_log = "data/processed_csv_files.txt"
        
    def _ensure_log_dir(self):
        """Ensure the log directory exists."""
        log_dir = Path(self.processed_files_log).parent
        log_dir.mkdir(parents=True, exist_ok=True)
    
    def _detect_csv_format(self, df: pd.DataFrame) -> str:
        """
        Detect CSV format based on columns.
        
        Returns:
            "colleague_format": user_id,embedding_vector,text_content,model_name,model_dim,created_at,text_hash
            "standard_format": document_id,embedding,model_name,vector_dim
        """
        columns = set(df.columns)
        
        colleague_indicators = {'user_id', 'embedding_vector', 'text_content'}
        standard_indicators = {'document_id', 'embedding'}
        
        if colleague_indicators.issubset(columns):
            return "colleague_format"
        elif standard_indicators.issubset(columns):
            return "standard_format"
        else:
            # Default to colleague format for now since that's what we have
            return "colleague_format"
    
    def _parse_embedding_vector(self, embedding_str: str) -> List[float]:
        """
        Parse embedding vector from CSV string format.
        
        Expected formats:
        - "[0.1, 0.2, 0.3, ...]"  (JSON array string)
        - "0.1,0.2,0.3,..."       (comma-separated values)
        """
        embedding_str = embedding_str.strip()
        
        try:
            # Try JSON format first
            if embedding_str.startswith('[') and embedding_str.endswith(']'):
                return json.loads(embedding_str)
            
            # Try comma-separated format
            elif ',' in embedding_str:
                return [float(x.strip()) for x in embedding_str.split(',')]
            
            # Single value (unlikely for embeddings but handle it)
            else:
                return [float(embedding_str)]
                
        except (json.JSONDecodeError, ValueError) as e:
            raise ValueError(f"Could not parse embedding vector '{embedding_str[:100]}...': {e}")
    
    def _validate_document_exists(self, document_id: str, session: Session) -> bool:
        """Check if document exists in the database."""
        try:
            doc_uuid = uuid.UUID(document_id)
            document = session.query(Document).filter(Document.id == doc_uuid).first()
            return document is not None
        except ValueError:
            return False
    
    def process_csv_file(
        self, 
        csv_file_path: str,
        skip_existing: bool = True,
        validate_documents: bool = True,
        batch_size: int = 100
    ) -> Dict[str, Any]:
        """
        Process a CSV file with embeddings and store them in PostgreSQL.
        
        Args:
            csv_file_path: Path to the CSV file
            skip_existing: Skip embeddings that already exist in DB
            validate_documents: Check if document_id exists before storing embedding
            batch_size: Number of records to process in each batch
            
        Returns:
            Processing results summary
        """
        session = self.session or next(get_session())
        
        try:
            if not os.path.exists(csv_file_path):
                raise FileNotFoundError(f"CSV file not found: {csv_file_path}")
            
            # Read CSV file
            df = pd.read_csv(csv_file_path)
            print(f"ğŸ“ Processing CSV file: {csv_file_path}")
            print(f"ğŸ“Š Total rows in CSV: {len(df)}")
            
            # Detect CSV format and validate columns
            csv_format = self._detect_csv_format(df)
            print(f"ğŸ“‹ Detected CSV format: {csv_format}")
            
            if csv_format == "colleague_format":
                # Format: user_id,embedding_vector,text_content,model_name,model_dim,created_at,text_hash
                required_columns = ['user_id', 'embedding_vector']
                id_col, embedding_col = 'user_id', 'embedding_vector'
            else:
                # Format: document_id,embedding,model_name,vector_dim
                required_columns = ['document_id', 'embedding']  
                id_col, embedding_col = 'document_id', 'embedding'
                
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise ValueError(f"Missing required columns in CSV: {missing_columns}. Found columns: {list(df.columns)}")
            
            # Process embeddings in batches
            results = {
                'total_rows': len(df),
                'processed': 0,
                'skipped': 0,
                'errors': 0,
                'error_details': [],
                'created_embeddings': []
            }
            
            for batch_start in range(0, len(df), batch_size):
                batch_end = min(batch_start + batch_size, len(df))
                batch_df = df.iloc[batch_start:batch_end]
                
                print(f"ğŸ”„ Processing batch {batch_start//batch_size + 1}: rows {batch_start+1}-{batch_end}")
                
                batch_embeddings = []
                
                for _, row in batch_df.iterrows():
                    try:
                        # Extract data from row based on format
                        document_id = str(row[id_col]).strip()
                        embedding_str = str(row[embedding_col]).strip()
                        model_name = str(row.get('model_name', 'colleague-script-v1.0')).strip()
                        
                        # Handle colleague format: user_id -> generate UUID for document_id
                        if csv_format == "colleague_format":
                            # For colleague format, we'll use user_id as document_id for now
                            # Later you can map this to actual document UUIDs if needed
                            document_id = f"USER_{document_id}"
                        
                        # Validate document_id format
                        if csv_format == "colleague_format":
                            # For colleague format, document_id is USER_xxxx - generate UUID
                            if not document_id.startswith("USER_"):
                                results['errors'] += 1
                                results['error_details'].append(f"Invalid user_id format: {document_id}")
                                continue
                            # Generate a consistent UUID for this user_id
                            doc_uuid = uuid.uuid5(uuid.NAMESPACE_OID, document_id)
                        else:
                            # Standard format - expect UUID
                            try:
                                doc_uuid = uuid.UUID(document_id)
                            except ValueError:
                                results['errors'] += 1
                                results['error_details'].append(f"Invalid document_id format: {document_id}")
                                continue
                        
                        # Validate document exists (optional - skip for colleague format since these are new user embeddings)
                        if validate_documents and csv_format != "colleague_format":
                            if not self._validate_document_exists(str(doc_uuid), session):
                                results['errors'] += 1
                                results['error_details'].append(f"Document not found in DB: {doc_uuid}")
                                continue
                        
                        # Check if embedding already exists (optional)
                        if skip_existing:
                            existing = session.query(Embedding).filter(
                                Embedding.document_id == doc_uuid,
                                Embedding.is_active == True
                            ).first()
                            
                            if existing:
                                results['skipped'] += 1
                                continue
                        
                        # Parse embedding vector
                        try:
                            embedding_vector = self._parse_embedding_vector(embedding_str)
                        except ValueError as e:
                            results['errors'] += 1
                            results['error_details'].append(f"Embedding parse error for {document_id}: {e}")
                            continue
                        
                        # Validate vector dimension
                        vector_dim = len(embedding_vector)
                        if vector_dim not in [384, 768, 1024, 1536]:
                            results['errors'] += 1
                            results['error_details'].append(f"Unsupported vector dimension {vector_dim} for {document_id}")
                            continue
                        
                        # Create embedding object
                        new_embedding = Embedding(
                            id=uuid.uuid4(),
                            document_id=doc_uuid,
                            embedding=embedding_vector,
                            model_name=model_name,
                            model_dim=vector_dim,
                            is_active=True
                        )
                        
                        batch_embeddings.append(new_embedding)
                        results['processed'] += 1
                        
                    except Exception as e:
                        results['errors'] += 1
                        results['error_details'].append(f"Row processing error: {e}")
                        continue
                
                # Store batch in database
                if batch_embeddings:
                    try:
                        session.add_all(batch_embeddings)
                        session.commit()
                        
                        # Log created embedding IDs (first few for reference)
                        batch_ids = [str(emb.id) for emb in batch_embeddings[:5]]
                        results['created_embeddings'].extend(batch_ids)
                        
                        print(f"âœ… Batch stored: {len(batch_embeddings)} embeddings")
                        
                    except Exception as e:
                        session.rollback()
                        results['errors'] += len(batch_embeddings)
                        results['error_details'].append(f"Batch storage error: {e}")
                        print(f"âŒ Batch storage failed: {e}")
            
            # Log processing summary
            print(f"\nğŸ“‹ Processing Summary:")
            print(f"   Total rows: {results['total_rows']}")
            print(f"   âœ… Processed: {results['processed']}")
            print(f"   â­ï¸  Skipped: {results['skipped']}")
            print(f"   âŒ Errors: {results['errors']}")
            
            if results['error_details']:
                print(f"\nâš ï¸  Error Details (first 5):")
                for error in results['error_details'][:5]:
                    print(f"   - {error}")
                if len(results['error_details']) > 5:
                    print(f"   ... and {len(results['error_details']) - 5} more errors")
            
            # Mark file as processed
            self._mark_file_processed(csv_file_path, results)
            
            return results
            
        except Exception as e:
            if session:
                session.rollback()
            raise e
        finally:
            if not self.session:  # Close session only if we created it
                session.close()
    
    def _mark_file_processed(self, csv_file_path: str, results: Dict[str, Any]):
        """Log processed file to avoid reprocessing."""
        self._ensure_log_dir()
        
        log_entry = f"{csv_file_path},{results['processed']},{results['errors']},{pd.Timestamp.now()}\n"
        
        with open(self.processed_files_log, 'a', encoding='utf-8') as f:
            f.write(log_entry)
    
    def get_processed_files(self) -> List[Dict[str, Any]]:
        """Get list of previously processed files."""
        if not os.path.exists(self.processed_files_log):
            return []
        
        processed_files = []
        with open(self.processed_files_log, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split(',')
                if len(parts) >= 4:
                    processed_files.append({
                        'file_path': parts[0],
                        'processed_count': int(parts[1]),
                        'error_count': int(parts[2]),
                        'processed_at': parts[3]
                    })
        
        return processed_files
    
    def process_directory(
        self, 
        directory_path: str,
        file_pattern: str = "*.csv",
        skip_processed: bool = True
    ) -> Dict[str, Any]:
        """
        Process all CSV files in a directory.
        
        Args:
            directory_path: Path to directory containing CSV files
            file_pattern: Glob pattern for CSV files (e.g., "embeddings_*.csv")
            skip_processed: Skip files that have been processed before
            
        Returns:
            Overall processing results
        """
        from glob import glob
        
        if not os.path.exists(directory_path):
            raise FileNotFoundError(f"Directory not found: {directory_path}")
        
        # Find CSV files
        csv_files = glob(os.path.join(directory_path, file_pattern))
        
        if not csv_files:
            return {
                'message': f'No CSV files found in {directory_path} matching pattern {file_pattern}',
                'files_processed': 0
            }
        
        # Get previously processed files if skip_processed=True
        processed_files_set = set()
        if skip_processed:
            processed_files = self.get_processed_files()
            processed_files_set = {pf['file_path'] for pf in processed_files}
        
        # Process each CSV file
        overall_results = {
            'total_files_found': len(csv_files),
            'files_processed': 0,
            'files_skipped': 0,
            'total_embeddings_processed': 0,
            'total_errors': 0,
            'file_results': []
        }
        
        for csv_file in csv_files:
            if skip_processed and csv_file in processed_files_set:
                print(f"â­ï¸  Skipping already processed file: {csv_file}")
                overall_results['files_skipped'] += 1
                continue
            
            try:
                file_results = self.process_csv_file(csv_file)
                
                overall_results['files_processed'] += 1
                overall_results['total_embeddings_processed'] += file_results['processed']
                overall_results['total_errors'] += file_results['errors']
                overall_results['file_results'].append({
                    'file': csv_file,
                    'results': file_results
                })
                
            except Exception as e:
                print(f"âŒ Failed to process file {csv_file}: {e}")
                overall_results['total_errors'] += 1
                overall_results['file_results'].append({
                    'file': csv_file,
                    'error': str(e)
                })
        
        print(f"\nğŸ‰ Directory Processing Complete:")
        print(f"   ğŸ“ Files found: {overall_results['total_files_found']}")
        print(f"   âœ… Files processed: {overall_results['files_processed']}")
        print(f"   â­ï¸  Files skipped: {overall_results['files_skipped']}")
        print(f"   ğŸ“Š Total embeddings: {overall_results['total_embeddings_processed']}")
        print(f"   âŒ Total errors: {overall_results['total_errors']}")
        
        return overall_results


# Utility functions for integration

def process_colleague_csv(
    csv_file_path: str,
    database_session: Optional[Session] = None
) -> Dict[str, Any]:
    """
    Simple function to process a single CSV file from colleague's script.
    
    Args:
        csv_file_path: Path to the CSV file with embeddings
        database_session: Optional database session (creates new if None)
        
    Returns:
        Processing results
    """
    processor = CSVEmbeddingProcessor(database_session)
    return processor.process_csv_file(csv_file_path)


def watch_and_process_csv_directory(
    watch_directory: str = "data/embeddings_csv",
    file_pattern: str = "embeddings_*.csv",
    check_interval: int = 30
):
    """
    Watch a directory for new CSV files and process them automatically.
    
    Args:
        watch_directory: Directory to monitor
        file_pattern: Pattern for CSV files to process
        check_interval: Check interval in seconds
    """
    import time
    
    print(f"ğŸ‘€ Watching directory: {watch_directory}")
    print(f"ğŸ” Looking for files matching: {file_pattern}")
    print(f"â° Check interval: {check_interval} seconds")
    print("Press Ctrl+C to stop watching...\n")
    
    processor = CSVEmbeddingProcessor()
    
    try:
        while True:
            try:
                results = processor.process_directory(
                    watch_directory, 
                    file_pattern, 
                    skip_processed=True
                )
                
                if results['files_processed'] > 0:
                    print(f"ğŸ”„ Processed {results['files_processed']} new files")
                
                time.sleep(check_interval)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Stopping file watcher...")
                break
            except Exception as e:
                print(f"âŒ Error during directory watch: {e}")
                time.sleep(check_interval)  # Continue watching despite errors
                
    except KeyboardInterrupt:
        print("\nâœ… File watcher stopped")


# CLI-style functions for manual processing

def process_single_csv_cli():
    """CLI function to process a single CSV file."""
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python -m backend.services.csv_embedding_processor <csv_file_path>")
        return
    
    csv_file_path = sys.argv[1]
    
    try:
        results = process_colleague_csv(csv_file_path)
        print(f"\nâœ… Processing complete!")
        print(f"ğŸ“Š Processed: {results['processed']} embeddings")
        print(f"â­ï¸  Skipped: {results['skipped']} embeddings")
        print(f"âŒ Errors: {results['errors']} embeddings")
        
    except Exception as e:
        print(f"âŒ Processing failed: {e}")


if __name__ == "__main__":
    process_single_csv_cli()