"""
Integration Script for External CSV Embedding Output

This script provides utilities to work with CSV files generated by 
external embedding scripts and process them into PostgreSQL.

Usage Examples:
1. Process a single CSV file
2. Watch a directory for new CSV files  
3. Manual batch processing
"""

import os
import pandas as pd
from pathlib import Path
import time
from typing import Dict, Any

try:
    # Try container path first (when running inside Docker)
    from app.services.csv_embedding_processor import (
        CSVEmbeddingProcessor, 
        process_external script_csv,
        watch_and_process_csv_directory
    )
except ImportError:
    # Fallback to host path (when running on host)
    from backend.app.services.csv_embedding_processor import (
        CSVEmbeddingProcessor, 
        process_external script_csv,
        watch_and_process_csv_directory
    )


def setup_data_directories():
    """Create necessary data directories for CSV processing."""
    directories = [
        "backend/data",
        "backend/data/embeddings_csv",
        "backend/data/processed_csv",
        "backend/data/logs"
    ]
    
    for dir_path in directories:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
        print(f"ðŸ“ Created directory: {dir_path}")


def process_external script_csv_simple(csv_filename: str) -> Dict[str, Any]:
    """
    Simple function to process external CSV file.
    
    Args:
        csv_filename: Name of CSV file in backend/data/embeddings_csv/
        
    Returns:
        Processing results
    """
    csv_path = f"backend/data/embeddings_csv/{csv_filename}"
    
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV file not found: {csv_path}")
    
    print(f"ðŸ”„ Processing CSV file: {csv_filename}")
    results = process_external script_csv(csv_path)
    
    print(f"âœ… Processing complete:")
    print(f"   ðŸ“Š Processed: {results['processed']} embeddings")
    print(f"   â­ï¸  Skipped: {results['skipped']} embeddings") 
    print(f"   âŒ Errors: {results['errors']} embeddings")
    
    return results


def start_csv_watcher():
    """Start watching for new CSV files from external script."""
    setup_data_directories()
    
    watch_directory = "backend/data/embeddings_csv"
    
    print(f"ðŸš€ Starting CSV file watcher...")
    print(f"ðŸ“ Watching: {watch_directory}")
    print(f"ðŸŽ¯ Pattern: embeddings_*.csv")
    print(f"â° Check interval: 30 seconds")
    print("\nPlace CSV files from external script in the watch directory.")
    print("Press Ctrl+C to stop...\n")
    
    watch_and_process_csv_directory(
        watch_directory=watch_directory,
        file_pattern="embeddings_*.csv", 
        check_interval=30
    )


def manual_batch_process():
    """Manually process all CSV files in the embeddings directory."""
    setup_data_directories()
    
    csv_dir = "backend/data/embeddings_csv"
    processor = CSVEmbeddingProcessor()
    
    print(f"ðŸ” Looking for CSV files in: {csv_dir}")
    
    results = processor.process_directory(
        directory_path=csv_dir,
        file_pattern="*.csv",
        skip_processed=True
    )
    
    print(f"\nðŸŽ‰ Batch processing complete:")
    print(f"   ðŸ“ Files found: {results['total_files_found']}")
    print(f"   âœ… Files processed: {results['files_processed']}")
    print(f"   â­ï¸  Files skipped: {results['files_skipped']}")
    print(f"   ðŸ“Š Total embeddings: {results['total_embeddings_processed']}")
    print(f"   âŒ Total errors: {results['total_errors']}")
    
    return results


def create_sample_csv_for_testing():
    """Create a sample CSV file to test the processing pipeline."""
    import uuid
    
    setup_data_directories()
    
    sample_data = []
    
    # Generate some sample data
    for i in range(5):
        doc_id = str(uuid.uuid4())
        # Generate random 384-dimensional embedding (normalized)
        import numpy as np
        embedding = np.random.randn(384)
        embedding = embedding / np.linalg.norm(embedding)  # Normalize
        embedding_str = f"[{','.join(map(str, embedding.tolist()))}]"
        
        sample_data.append({
            'document_id': doc_id,
            'embedding': embedding_str,
            'model_name': 'test-sentence-transformer',
            'vector_dim': 384
        })
    
    # Save to CSV
    df = pd.DataFrame(sample_data)
    csv_path = "backend/data/embeddings_csv/test_embeddings_sample.csv"
    df.to_csv(csv_path, index=False)
    
    print(f"ðŸ“„ Created sample CSV: {csv_path}")
    print(f"ðŸ“Š Sample contains {len(sample_data)} test embeddings")
    print("\nSample CSV format:")
    print(df.head(2).to_string(index=False))
    
    return csv_path


def validate_csv_format(csv_path: str) -> Dict[str, Any]:
    """
    Validate that a CSV file matches the expected format for processing.
    
    Args:
        csv_path: Path to CSV file
        
    Returns:
        Validation results
    """
    if not os.path.exists(csv_path):
        return {"valid": False, "error": f"File not found: {csv_path}"}
    
    try:
        df = pd.read_csv(csv_path)
        
        # Check required columns
        required_columns = ['document_id', 'embedding']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            return {
                "valid": False, 
                "error": f"Missing required columns: {missing_columns}",
                "found_columns": list(df.columns)
            }
        
        # Check data types and sample values
        validation_results = {
            "valid": True,
            "total_rows": len(df),
            "columns": list(df.columns),
            "sample_document_ids": df['document_id'].head(3).tolist(),
            "embedding_format_check": []
        }
        
        # Check embedding format for first few rows
        for i, embedding_str in enumerate(df['embedding'].head(3)):
            try:
                if str(embedding_str).startswith('[') and str(embedding_str).endswith(']'):
                    validation_results["embedding_format_check"].append(f"Row {i+1}: JSON array format âœ…")
                elif ',' in str(embedding_str):
                    validation_results["embedding_format_check"].append(f"Row {i+1}: Comma-separated format âœ…")
                else:
                    validation_results["embedding_format_check"].append(f"Row {i+1}: Unknown format âš ï¸")
            except Exception as e:
                validation_results["embedding_format_check"].append(f"Row {i+1}: Error - {e}")
        
        return validation_results
        
    except Exception as e:
        return {"valid": False, "error": f"CSV reading error: {e}"}


def show_processing_status():
    """Show current processing status and statistics."""
    processor = CSVEmbeddingProcessor()
    
    print("ðŸ“‹ CSV Processing Status Report")
    print("=" * 40)
    
    # Get processed files log
    processed_files = processor.get_processed_files()
    
    if processed_files:
        print(f"ðŸ“ Previously processed files: {len(processed_files)}")
        print("\nRecent files:")
        for pf in processed_files[-5:]:  # Show last 5
            print(f"   {pf['file_path']} - {pf['processed_count']} embeddings ({pf['processed_at']})")
    else:
        print("ðŸ“ No files processed yet")
    
    # Check for new files in watch directory
    csv_dir = "backend/data/embeddings_csv"
    if os.path.exists(csv_dir):
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
        print(f"\nðŸ“‚ Files in watch directory: {len(csv_files)}")
        for csv_file in csv_files[:5]:  # Show first 5
            print(f"   {csv_file}")
        if len(csv_files) > 5:
            print(f"   ... and {len(csv_files) - 5} more files")
    else:
        print(f"\nðŸ“‚ Watch directory not found: {csv_dir}")


# Main CLI interface

def main():
    """Main CLI interface for CSV processing."""
    import sys
    
    if len(sys.argv) < 2:
        print("ðŸ”§ CSV Embedding Processor")
        print("=" * 30)
        print("Usage:")
        print("  python csv_integration.py setup          - Create data directories")
        print("  python csv_integration.py watch          - Start CSV file watcher")
        print("  python csv_integration.py process <file> - Process single CSV file")
        print("  python csv_integration.py batch          - Process all CSV files")
        print("  python csv_integration.py sample         - Create sample CSV")
        print("  python csv_integration.py validate <file>- Validate CSV format")
        print("  python csv_integration.py status         - Show processing status")
        return
    
    command = sys.argv[1].lower()
    
    try:
        if command == "setup":
            setup_data_directories()
            
        elif command == "watch":
            start_csv_watcher()
            
        elif command == "process":
            if len(sys.argv) < 3:
                print("âŒ Please specify CSV filename")
                return
            filename = sys.argv[2]
            process_external script_csv_simple(filename)
            
        elif command == "batch":
            manual_batch_process()
            
        elif command == "sample":
            csv_path = create_sample_csv_for_testing()
            print(f"\nðŸ§ª Test the sample with:")
            print(f"   python csv_integration.py process {os.path.basename(csv_path)}")
            
        elif command == "validate":
            if len(sys.argv) < 3:
                print("âŒ Please specify CSV file path")
                return
            csv_path = sys.argv[2]
            results = validate_csv_format(csv_path)
            
            if results["valid"]:
                print("âœ… CSV format is valid")
                print(f"   Rows: {results['total_rows']}")
                print(f"   Columns: {results['columns']}")
                for check in results["embedding_format_check"]:
                    print(f"   {check}")
            else:
                print(f"âŒ CSV format validation failed: {results['error']}")
                
        elif command == "status":
            show_processing_status()
            
        else:
            print(f"âŒ Unknown command: {command}")
            
    except Exception as e:
        print(f"âŒ Error: {e}")


if __name__ == "__main__":
    main()


# Integration workflow documentation

"""
=== external script INTEGRATION WORKFLOW ===

1. Setup (one time):
   python csv_integration.py setup

2. external script runs embedding script:
   - Script generates CSV files with format: document_id,embedding,model_name
   - Files saved to backend/data/embeddings_csv/

3. Process CSV files:
   
   Option A - Manual processing:
   python csv_integration.py process embeddings_20231107.csv
   
   Option B - Batch processing:
   python csv_integration.py batch
   
   Option C - Automatic watching:
   python csv_integration.py watch

4. Verify processing:
   python csv_integration.py status

5. Use API for similarity search:
   POST /api/embeddings/search/similar
   POST /api/embeddings/search/job-match

=== CSV FORMAT EXPECTED ===

document_id,embedding,model_name,vector_dim
uuid-123,"[0.1,0.2,0.3,...]",sentence-transformers/all-MiniLM-L12-v2,384
uuid-456,"[0.4,0.5,0.6,...]",sentence-transformers/all-MiniLM-L12-v2,384

Notes:
- embedding column can be JSON array "[0.1,0.2,...]" or comma-separated "0.1,0.2,..."
- document_id must be valid UUID of existing document in database
- model_name is optional (defaults to "external script-script-v1.0")
- vector_dim is auto-detected from embedding length

=== API ENDPOINTS FOR CSV PROCESSING ===

POST /api/embeddings/process-csv
- Body: {"csv_file_path": "embeddings_20231107.csv"}

POST /api/embeddings/process-directory  
- Body: {"directory_path": "embeddings_csv", "file_pattern": "*.csv"}

GET /api/embeddings/processed-files
- Returns list of previously processed files

"""
