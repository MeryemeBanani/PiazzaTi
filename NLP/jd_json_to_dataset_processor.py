# -*- coding: utf-8 -*-
"""jd_json_to_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tURID_dJeuES4L2rIszbYiZ-cMfQH8Wc
"""

"""
jd_to_csv_processor.py
Converte file JSON JD in un dataset CSV tabulare
Con controllo duplicati e pulizia JD eliminate
"""

import json

from pathlib import Path
from typing import List, Dict, Set
from datetime import datetime
import pandas as pd


INPUT_FOLDER = "/var/lib/docker/piazzati-data/jds"
OUTPUT_FOLDER = "Dataset"
OUTPUT_FILENAME = "jd_dataset.csv"

ARRAY_SEP = " | "
LIST_SEP = ", "


def get_active_jd_ids(input_path: Path) -> Set[str]:
    """
    Estrae tutti i jd_id dai file JSON presenti nella cartella.
    """
    active_jds = set()

    json_files = list(input_path.glob("*.json"))

    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            jd_id = str(data.get('jd_id', '')) if data.get('jd_id') else ''
            if jd_id:
                active_jds.add(jd_id)
        except:
            continue

    return active_jds


def clean_deleted_jds(output_path: Path, active_jd_ids: Set[str]) -> int:
    """
    Rimuove dal dataset le JD che non hanno più file JSON.
    """
    if not output_path.exists():
        return 0

    try:
        df = pd.read_csv(output_path, encoding='utf-8')

        if 'jd_id' not in df.columns:
            print("  Colonna jd_id non trovata nel dataset")
            return 0

        original_count = len(df)

        deleted_jds = []
        for idx, jd_id in enumerate(df['jd_id']):
            if pd.notna(jd_id) and str(jd_id).strip():
                if str(jd_id) not in active_jd_ids:
                    deleted_jds.append(str(jd_id))

        if deleted_jds:
            df = df[df['jd_id'].isin(active_jd_ids) | df['jd_id'].isna()]
            df.to_csv(output_path, index=False, encoding='utf-8')

            removed_count = original_count - len(df)

            print(f"  JD rimosse: {len(set(deleted_jds))}")
            print(f"  Righe eliminate: {removed_count}")

            if len(deleted_jds) <= 10:
                for jd_id in set(deleted_jds):
                    print(f"    - {jd_id}")

            return removed_count
        else:
            print("  Nessuna JD da rimuovere")
            return 0

    except Exception as e:
        print(f"  Errore durante la pulizia: {e}")
        return 0


def get_existing_jd_ids(output_path: Path) -> Dict[str, int]:
    """
    Legge il CSV esistente e restituisce i jd_id già presenti.
    """
    if not output_path.exists():
        return {}

    try:
        df = pd.read_csv(output_path, encoding='utf-8')

        jd_id_to_index = {}
        if 'jd_id' in df.columns:
            for idx, jd_id in enumerate(df['jd_id']):
                if pd.notna(jd_id) and str(jd_id).strip():
                    jd_id_to_index[str(jd_id)] = idx

        return jd_id_to_index

    except Exception as e:
        print(f"Avviso: impossibile leggere il CSV esistente: {e}")
        return {}


def extract_jd_id_from_json(json_path: Path) -> str:
    """
    Estrae il jd_id da un file JSON.
    """
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        jd_id = str(data.get('jd_id', '')) if data.get('jd_id') else ''
        return jd_id

    except Exception as e:
        print(f"  Errore lettura da {json_path.name}: {e}")
        return ''


def flatten_location(data: Dict) -> Dict:
    """
    Appiattisce il campo location.
    """
    if not data:
        return {
            'location_city': '',
            'location_country': '',
            'location_remote': ''
        }

    return {
        'location_city': data.get('city', ''),
        'location_country': data.get('country', ''),
        'location_remote': 'Yes' if data.get('remote') else 'No'
    }


def flatten_constraints(data: Dict) -> Dict:
    """
    Appiattisce il campo constraints.
    """
    if not data:
        return {
            'constraints_visa': '',
            'constraints_relocation': '',
            'constraints_seniority': '',
            'constraints_languages': ''
        }

    languages = []
    if 'languages_min' in data and data['languages_min']:
        for lang in data['languages_min']:
            lang_name = lang.get('lang', '')
            level = lang.get('level', '')
            languages.append(f"{lang_name} ({level})")

    return {
        'constraints_visa': 'Required' if data.get('visa') else 'Not required',
        'constraints_relocation': 'Available' if data.get('relocation') else 'Not available',
        'constraints_seniority': data.get('seniority', ''),
        'constraints_languages': LIST_SEP.join(languages)
    }


def flatten_dei_requirements(data: Dict) -> Dict:
    """
    Appiattisce il campo dei_requirements.
    """
    if not data or 'target_balance' not in data:
        return {
            'dei_gender_target': '',
            'dei_underrepresented_target': ''
        }

    target = data.get('target_balance', {})

    return {
        'dei_gender_target': str(target.get('gender', '')),
        'dei_underrepresented_target': str(target.get('underrepresented', ''))
    }


def flatten_metadata(data: Dict) -> Dict:
    """
    Appiattisce il campo metadata.
    """
    if not data:
        return {
            'salary_min': '',
            'salary_max': '',
            'salary_currency': '',
            'contract': ''
        }

    salary_range = data.get('salary_range', {})

    return {
        'salary_min': str(salary_range.get('min', '')) if salary_range else '',
        'salary_max': str(salary_range.get('max', '')) if salary_range else '',
        'salary_currency': salary_range.get('currency', '') if salary_range else '',
        'contract': data.get('contract', '')
    }


def json_to_row(data: Dict, source_file: str) -> Dict:
    """
    Trasforma un JSON JD in una riga del dataset.
    """
    row = {
        'jd_id': data.get('jd_id') or source_file.replace('.json', ''),
        'source_file': source_file,
        'processed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'title': data.get('title', ''),
        'department': data.get('department', ''),
        'description': data.get('description', ''),
        'requirements': LIST_SEP.join(data.get('requirements', [])),
        'nice_to_have': LIST_SEP.join(data.get('nice_to_have', []))
    }

    row.update(flatten_location(data.get('location', {})))
    row.update(flatten_constraints(data.get('constraints', {})))
    row.update(flatten_dei_requirements(data.get('dei_requirements', {})))
    row.update(flatten_metadata(data.get('metadata', {})))

    return row


def process_files(input_dir: str, output_dir: str, output_file: str):
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    full_output_path = output_path / output_file

    if not input_path.exists():
        print(f"Errore: cartella {input_dir} non trovata")
        return False

    json_files = list(input_path.glob("*.json"))

    if not json_files:
        print(f"Nessun file JSON trovato in {input_dir}")
        return False

    print(f"Trovati {len(json_files)} file JSON")

    print(f"\nControllo JD eliminate...")
    active_jd_ids = get_active_jd_ids(input_path)
    print(f"  JD attive nei JSON: {len(active_jd_ids)}")

    removed_count = clean_deleted_jds(full_output_path, active_jd_ids)

    print(f"\nControllo duplicati nel dataset esistente...")
    jd_id_to_index = get_existing_jd_ids(full_output_path)

    if jd_id_to_index:
        print(f"  JD ID unici: {len(jd_id_to_index)}")
    else:
        print(f"  Nessun dataset esistente")

    print(f"\nAnalisi file JSON...")

    files_to_add = []
    files_to_update = []
    files_skipped = []
    files_no_id = []

    for json_file in json_files:
        jd_id = extract_jd_id_from_json(json_file)

        if not jd_id:
            files_no_id.append(json_file.name)
            files_to_add.append(json_file)

        elif jd_id in jd_id_to_index:
            files_to_update.append((json_file, jd_id, jd_id_to_index[jd_id]))
            print(f"  UPDATE: {json_file.name} -> {jd_id}")

        else:
            files_to_add.append(json_file)

    print(f"\nRisultato:")
    print(f"  Nuove: {len(files_to_add)}")
    print(f"  Da aggiornare: {len(files_to_update)}")
    if files_no_id:
        print(f"  Senza jd_id: {len(files_no_id)}")

    if not files_to_add and not files_to_update:
        print("\nNessun file da processare")
        if removed_count > 0:
            print(f"Dataset aggiornato: {removed_count} righe eliminate")
        return True

    rows_to_add = []
    errors = []

    if files_to_add:
        print(f"\nProcessamento {len(files_to_add)} nuove JD...")

        for json_file in files_to_add:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                row = json_to_row(data, json_file.name)
                rows_to_add.append(row)

                jd_id = row['jd_id'] or 'N/A'
                print(f"  {json_file.name} -> {jd_id}")

            except Exception as e:
                errors.append((json_file.name, str(e)))
                print(f"  ERRORE: {json_file.name} - {e}")

    rows_to_update = []

    if files_to_update:
        print(f"\nAggiornamento {len(files_to_update)} JD esistenti...")

        for json_file, jd_id, row_index in files_to_update:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                row = json_to_row(data, json_file.name)
                rows_to_update.append((row_index, row))

                print(f"  {json_file.name} -> {jd_id}")

            except Exception as e:
                errors.append((json_file.name, str(e)))
                print(f"  ERRORE: {json_file.name} - {e}")

    output_path.mkdir(parents=True, exist_ok=True)

    if full_output_path.exists():
        existing_df = pd.read_csv(full_output_path, encoding='utf-8')

        for row_index, row_data in rows_to_update:
            for col, value in row_data.items():
                if col in existing_df.columns:
                    existing_df.at[row_index, col] = value

        if rows_to_add:
            new_df = pd.DataFrame(rows_to_add)
            final_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            final_df = existing_df
    else:
        if rows_to_add:
            final_df = pd.DataFrame(rows_to_add)
        else:
            print("Nessuna riga da salvare")
            return False

    cols = [
        'jd_id', 'source_file', 'processed_at', 'title', 'department',
        'location_city', 'location_country', 'location_remote',
        'description', 'requirements', 'nice_to_have',
        'constraints_visa', 'constraints_relocation', 'constraints_seniority', 'constraints_languages',
        'dei_gender_target', 'dei_underrepresented_target',
        'salary_min', 'salary_max', 'salary_currency', 'contract'
    ]

    existing_cols = [c for c in cols if c in final_df.columns]
    final_df = final_df[existing_cols]

    final_df.to_csv(full_output_path, index=False, encoding='utf-8')

    print(f"\nCompletato: {full_output_path}")
    print(f"Nuove righe: {len(rows_to_add)}")
    print(f"Righe aggiornate: {len(rows_to_update)}")
    if removed_count > 0:
        print(f"Righe eliminate: {removed_count}")
    print(f"Totale righe: {len(final_df)}")

    if errors:
        print(f"\nErrori: {len(errors)}")
        for fname, error in errors:
            print(f"  {fname}: {error}")

    return True


if __name__ == "__main__":
    process_files(INPUT_FOLDER, OUTPUT_FOLDER, OUTPUT_FILENAME)